import os
import time
import gradio
import asyncio
import fitz  # PyMuPDF
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_classic.chains import RetrievalQA
from langchain_classic.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import re

from functools import lru_cache
import diskcache as dc
import hashlib


from langchain_classic.output_parsers import StructuredOutputParser, ResponseSchema

# ------------------- 1. ç¯å¢ƒ & è·¯å¾„ -------------------
load_dotenv()
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))   # é¡¹ç›®æ ¹ç›®å½•
print(f"base_dir: {base_dir}")

OUTPUT_DIR = os.path.join(base_dir, os.getenv("OUTPUT_DIR", "output/"))
print(f"OUTPUT_DIR: {OUTPUT_DIR}")

persist_directory = os.path.join(OUTPUT_DIR, "chroma_db")
api_key = os.getenv("DEEPSEEK_API_KEY")
print(f"API Key loaded: {'Yes' if api_key else 'No'}")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ------------------- 2. åˆ›å»º/åŠ è½½ Chroma å‘é‡åº“ -------------------
if not os.path.exists(persist_directory):
    print("æ­£åœ¨æ„å»ºæ–°çš„ Chroma æ•°æ®åº“...")
    chunks = []
    texts = []
    metadatas = []

    if os.path.isdir(OUTPUT_DIR):
        # éå† output ç›®å½•ä¸‹çš„æ‰€æœ‰ txt åˆ†å—
        for fn in sorted(os.listdir(OUTPUT_DIR)):
            if fn.startswith("chunk_") and fn.endswith(".txt"):
                path = os.path.join(OUTPUT_DIR, fn)

                # è·å–ä¸å¸¦åç¼€çš„ IDï¼Œä¾‹å¦‚ chunk_Chapter_2_0
                chunk_id = fn.replace(".txt", "")

                with open(path, "r", encoding="utf-8") as f:
                    content = f.read()


                    text_with_id = f"æ¥æºID: {chunk_id}\n{content}"

                    texts.append(text_with_id)
                    metadatas.append({"source": fn})

        if not texts:
            raise FileNotFoundError(f"{OUTPUT_DIR} ä¸­æœªæ‰¾åˆ° chunk_*.txtï¼Œè¯·å…ˆç¡®ä¿è¿è¡Œäº† init.py")
    else:
        raise FileNotFoundError(f"{OUTPUT_DIR} ä¸å­˜åœ¨")

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metadatas,
        persist_directory=persist_directory
    )
    print(f"Chroma æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå…±è½½å…¥ {len(texts)} ä¸ªåˆ†å—")
else:
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    print("Chroma æ•°æ®åº“å·²åŠ è½½")


# æŒä¹…åŒ–ç£ç›˜ç¼“å­˜
CACHE_DIR = os.path.join(OUTPUT_DIR, "rag_cache")
os.makedirs(CACHE_DIR, exist_ok=True)
rag_cache = dc.Cache(CACHE_DIR, disk_min_file_size=0)  # æŒä¹…åŒ–ç£ç›˜ç¼“å­˜
print(f"RAG ç¼“å­˜ç›®å½•: {CACHE_DIR}")

# ------------------- 3. DeepSeek LLM -------------------
if not api_key:
    raise ValueError("DEEPSEEK_API_KEY æœªè®¾ç½®")

llm = ChatOpenAI(
    openai_api_key=api_key,
    openai_api_base="https://api.deepseek.com/v1",
    model="deepseek-chat",
    temperature=0.1,
    max_tokens=500
)
print("DeepSeek LLM é…ç½®æˆåŠŸ")

# ------------------- 4. Prompt -------------------
prompt_template = """
ä½ æ˜¯è¯¾ç¨‹å­¦ä¹ åŠ©æ‰‹ã€‚

æŒ‰ä»¥ä¸‹æ­¥éª¤æ¨ç†ï¼š
1. è¯†åˆ«ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚
2. æ€»ç»“æˆ–æ¨æµ‹ç­”æ¡ˆã€‚
3. æ³¨æ˜æ¥æºå—ç¼–å·ï¼ˆå¦‚ chunk_3ï¼‰ã€‚

ç¤ºä¾‹ï¼š
- é—®é¢˜: ä»€ä¹ˆæ˜¯æ•æ·å¼€å‘ï¼Ÿ
- ä¸Šä¸‹æ–‡: chunk_Chapter_2_10: Agile means iterative development...
- ç­”æ¡ˆ: æ•æ·å¼€å‘æ˜¯ä¸€ç§è¿­ä»£å’Œå¢é‡å¼€å‘çš„è½¯ä»¶å¼€å‘æ–¹æ³•ã€‚
- æ¥æº: chunk_Chapter_2_10

åŸºäºä»¥ä¸‹è¯¾ç¨‹ä¸Šä¸‹æ–‡ï¼Œå›ç­”é—®é¢˜ã€‚ä¼˜å…ˆæå–å…³é”®ä¿¡æ¯ï¼Œè‹¥ä¿¡æ¯ä¸è¶³ï¼Œå¯åŸºäºç›¸å…³æ¦‚å¿µæ¨æµ‹æˆ–æ€»ç»“ï¼Œå¹¶æ³¨æ˜æ¥æºå—ç¼–å·ã€‚è‹¥å®Œå…¨æ— æ³•å›ç­”ï¼Œè¯´â€œæœªçŸ¥â€ã€‚

ä¸Šä¸‹æ–‡: {context}
é—®é¢˜: {question}
ç­”æ¡ˆ:
"""
PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# ------------------- 5. MMR æ£€ç´¢å™¨ï¼ˆåŠ¨æ€ kï¼‰ -------------------
retriever = vectorstore.as_retriever(
    search_type="mmr",                 # Maximum Marginal Relevance
    search_kwargs={"k": 15, "fetch_k": 30}   # k=15 æœ€ç»ˆè¿”å›ï¼Œfetch_k=30 å€™é€‰æ± 
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT}
)


def cached_rag(query: str) -> str:
    """
    æŒä¹…åŒ–ç£ç›˜ç¼“å­˜ RAG é—®ç­”ï¼ˆè·¨è¿›ç¨‹ç”Ÿæ•ˆï¼‰
    - ç¬¬ä¸€æ¬¡è¿è¡Œï¼šå®Œæ•´ RAG â†’ å­˜å…¥ç£ç›˜
    - ç¬¬äºŒæ¬¡è¿è¡Œï¼šç›´æ¥ä»ç£ç›˜è¯»å– â†’ 0.01s
    """
    # æ ‡å‡†åŒ–æŸ¥è¯¢ + ç”Ÿæˆå”¯ä¸€ key
    normalized = query.strip().lower()
    cache_key = f"rag_v1:{hashlib.md5(normalized.encode()).hexdigest()}"

    if cache_key in rag_cache:
        print(f"[ç¼“å­˜å‘½ä¸­] {cache_key[-8:]}")
        return rag_cache[cache_key]

    print(f"[ç¼“å­˜æœªå‘½ä¸­] æ‰§è¡Œ RAG...")
    result = qa_chain.invoke({"query": query})["result"]
    rag_cache[cache_key] = result
    return result


# ==============================================================================
# ------------------- æ–°å¢æ¨¡å—ï¼šä¸ªæ€§åŒ–ç»ƒä¹ ç”Ÿæˆ (Quiz Generation) -------------------
# ==============================================================================

# 1. ä¼˜åŒ– Schema å®šä¹‰ï¼šæ˜ç¡®å‘Šè¯‰ LLM é€‰é¡¹æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬åˆ—è¡¨
response_schemas = [
    ResponseSchema(name="question", description="é¢˜ç›®å†…å®¹ï¼Œå¿…é¡»æ¸…æ™°å®Œæ•´"),
    ResponseSchema(name="options",
                   description="åŒ…å«4ä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼ˆList[str]ï¼‰ï¼Œåˆ†åˆ«ä»£è¡¨Aã€Bã€Cã€Då››ä¸ªé€‰é¡¹çš„å…·ä½“æè¿°ã€‚æ³¨æ„ï¼šä¸è¦åœ¨å­—ç¬¦ä¸²é‡ŒåŒ…å« 'A.' æˆ– '1.' ç­‰å‰ç¼€ï¼Œåªå†™å†…å®¹ã€‚"),
    ResponseSchema(name="answer", description="æ­£ç¡®é€‰é¡¹çš„å­—æ¯ï¼Œä»…è¾“å‡º 'A', 'B', 'C', or 'D'"),
    ResponseSchema(name="explanation", description="ç­”æ¡ˆè§£æï¼Œè§£é‡Šæ­£ç¡®åŸå› åŠå¹²æ‰°é¡¹é”™è¯¯åŸå› ")
]

# åˆå§‹åŒ–è§£æå™¨
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()


def generate_quiz_func(topic: str):
    """
    åŠŸèƒ½ï¼šåŸºäºè¾“å…¥çš„çŸ¥è¯†ç‚¹ï¼Œæ£€ç´¢è¯¾ç¨‹å†…å®¹ï¼Œç”Ÿæˆä¸€é“å•é€‰é¢˜ã€‚
    """
    if not topic or not topic.strip():
        return "âš ï¸ è¯·è¾“å…¥ä¸€ä¸ªå…·ä½“çš„çŸ¥è¯†ç‚¹ï¼Œä¾‹å¦‚ï¼š'æ•æ·å¼€å‘' æˆ– 'é¡¹ç›®ç”Ÿå‘½å‘¨æœŸ'ã€‚"

    print(f"ğŸ“ æ­£åœ¨ä¸ºçŸ¥è¯†ç‚¹ '{topic}' ç”Ÿæˆç»ƒä¹ é¢˜...")

    try:
        # 1. æ£€ç´¢ç´ æ
        docs = retriever.invoke(topic)

        # å®¹é”™å¤„ç†ï¼šå¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ï¼Œç»™ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œè®© Prompt å†³å®šæ€ä¹ˆåš
        if docs:
            context_text = "\n".join([d.page_content for d in docs[:3]])
        else:
            context_text = "ï¼ˆæœªæ£€ç´¢åˆ°å…·ä½“è¯¾ç¨‹å†…å®¹ï¼Œè¯·åŸºäºè¯¥çŸ¥è¯†ç‚¹çš„é€šç”¨æ¦‚å¿µå‡ºé¢˜ï¼‰"

        # 2. æ„å»º Prompt
        # ä¼˜åŒ–ç­–ç•¥ï¼š
        quiz_template = """
        ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¤§å­¦è¯¾ç¨‹å‡ºé¢˜è€å¸ˆã€‚è¯·é’ˆå¯¹ç›®æ ‡ã€çŸ¥è¯†ç‚¹ã€‘å‡ºä¸€é“å•é¡¹é€‰æ‹©é¢˜ã€‚

        ã€å‚è€ƒè¯¾ç¨‹å†…å®¹ã€‘ï¼š
        {context}

        ã€ç›®æ ‡çŸ¥è¯†ç‚¹ã€‘ï¼š{topic}

        ã€å‡ºé¢˜è¦æ±‚ã€‘ï¼š
        1. ä¼˜å…ˆä¾æ®ã€å‚è€ƒè¯¾ç¨‹å†…å®¹ã€‘å‡ºé¢˜ã€‚å¦‚æœå†…å®¹ä¸­æœªåŒ…å«å…·ä½“ç»†èŠ‚ï¼ˆå¦‚ä»…æœ‰æ ‡é¢˜ï¼‰ï¼Œè¯·åŸºäºä½ å¯¹è¯¥ã€ç›®æ ‡çŸ¥è¯†ç‚¹ã€‘çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¡¥å…¨ï¼Œç¡®ä¿é¢˜ç›®é€»è¾‘é€šé¡ºã€‚
        2. é¢˜ç›®éš¾åº¦é€‚ä¸­ï¼Œé€‚åˆå¤§å­¦ç”Ÿå¤ä¹ ã€‚
        3. é€‰é¡¹ï¼ˆoptionsï¼‰å¿…é¡»æ˜¯åŒ…å«4ä¸ªå…·ä½“æè¿°çš„åˆ—è¡¨ï¼Œä¸è¦åŒ…å« "A." ç­‰å‰ç¼€ã€‚
        4. å¿…é¡»ä¸¥æ ¼éµå®ˆä¸‹æ–¹çš„ JSON æ ¼å¼è¾“å‡ºã€‚

        {format_instructions}
        """

        prompt = PromptTemplate(
            template=quiz_template,
            input_variables=["context", "topic"],
            partial_variables={"format_instructions": format_instructions}
        )

        # 3. è°ƒç”¨ LLM ç”Ÿæˆ
        chain = prompt | llm
        response = chain.invoke({"context": context_text, "topic": topic})

        # 4. è§£æç»“æœå¹¶æ ¼å¼åŒ–
        try:
            # è§£æ LLM è¿”å›çš„ JSON
            data = output_parser.parse(response.content)

            # å®¹é”™æ£€æŸ¥ï¼šç¡®ä¿ options æœ‰ 4 ä¸ª
            opts = data.get('options', [])
            while len(opts) < 4:
                opts.append("ï¼ˆç”Ÿæˆé€‰é¡¹ä¸è¶³ï¼‰")

            # æ ¼å¼åŒ–è¾“å‡º
            display_text = (
                f"### ğŸ¯ ä¸ªæ€§åŒ–ç»ƒä¹ é¢˜\n\n"
                f"**â“ é¢˜ç›®**: {data['question']}\n\n"
                f"**é€‰é¡¹**:\n"
                f"A. {opts[0]}\n"
                f"B. {opts[1]}\n"
                f"C. {opts[2]}\n"
                f"D. {opts[3]}\n\n"
                f"---\n"
                f"**âœ… å‚è€ƒç­”æ¡ˆ**: {data['answer']}\n\n"
                f"**ğŸ’¡ è§£æ**: {data['explanation']}\n"
            )
            return display_text

        except Exception as parse_err:
            print(f"JSON è§£æå¤±è´¥: {parse_err}")
            return f"âš ï¸ é¢˜ç›®ç”Ÿæˆæ•°æ®è§£æé”™è¯¯ï¼Œè¯·é‡è¯•ã€‚\n\nåŸå§‹è¿”å›:\n{response.content}"

    except Exception as e:
        return f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"


# ==============================================================================
# ------------------- æ–°å¢æ¨¡å—ï¼šæ–‡ä»¶ä¸Šä¼ å¤„ç† (File Upload) -------------------
# ==============================================================================

def preprocess_pdf(pdf_path: str):
    """å¤„ç† PDF æ–‡ä»¶å¹¶è¿”å›åˆ†å—æ–‡æœ¬åˆ—è¡¨"""
    print(f"æ­£åœ¨å¤„ç† PDF: {pdf_path}")
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    
    # æ¸…æ´—é€»è¾‘
    cleaned_text = re.sub(r'\n+', ' ', text)
    cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned_text)
    
    # åˆ†å—
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_text(cleaned_text)
    
    return chunks, os.path.basename(pdf_path)


def preprocess_srt(srt_path: str):
    """å¤„ç† SRT å­—å¹•æ–‡ä»¶å¹¶è¿”å›åˆ†å—æ–‡æœ¬åˆ—è¡¨"""
    print(f"æ­£åœ¨å¤„ç†å­—å¹•: {srt_path}")
    
    with open(srt_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    text_content = []
    for line in lines:
        line = line.strip()
        # è·³è¿‡çº¯æ•°å­—ï¼ˆå­—å¹•åºå·ï¼‰
        if line.isdigit():
            continue
        # è·³è¿‡æ—¶é—´è½´
        if '-->' in line:
            continue
        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue
        text_content.append(line)
    
    # åˆå¹¶ä¸ºå®Œæ•´æ–‡æœ¬
    full_text = " ".join(text_content)
    cleaned_text = re.sub(r'\s+', ' ', full_text)
    
    # åˆ†å—
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_text(cleaned_text)
    
    return chunks, os.path.basename(srt_path)


def add_documents_to_vectorstore(chunks, source_filename):
    """å°†æ–°çš„æ–‡æ¡£åˆ†å—æ·»åŠ åˆ°ç°æœ‰çš„å‘é‡åº“ä¸­"""
    global vectorstore, retriever, qa_chain
    
    # å‡†å¤‡æ–‡æœ¬å’Œå…ƒæ•°æ®
    texts = []
    metadatas = []
    base_name = os.path.splitext(os.path.basename(source_filename))[0]
    base_name = base_name.replace(" ", "_")
    
    for i, chunk in enumerate(chunks):
        chunk_id = f"chunk_{base_name}_{i}"
        text_with_id = f"æ¥æºID: {chunk_id}\n{chunk}"
        texts.append(text_with_id)
        metadatas.append({"source": f"{chunk_id}.txt"})
        
        # åŒæ—¶ä¿å­˜åˆ° output ç›®å½•
        out_name = f"{chunk_id}.txt"
        with open(os.path.join(OUTPUT_DIR, out_name), "w", encoding="utf-8") as f:
            f.write(chunk)
    
    # æ·»åŠ åˆ°å‘é‡åº“
    if texts:
        vectorstore.add_texts(texts=texts, metadatas=metadatas)
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªåˆ†å—åˆ°å‘é‡åº“")
        
        # æ›´æ–°æ£€ç´¢å™¨å’Œ QA é“¾
        retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 15, "fetch_k": 30}
        )
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT}
        )
    
    return len(texts)


def handle_file_upload(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    if uploaded_file is None:
        return "âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ PDF æˆ– SRT æ ¼å¼ï¼‰"
    
    try:
        # Gradio 4.44.1 ä¸­ File ç»„ä»¶è¿”å›æ–‡ä»¶å¯¹è±¡ï¼Œéœ€è¦è·å– name å±æ€§
        if isinstance(uploaded_file, str):
            file_path = uploaded_file
        elif hasattr(uploaded_file, 'name'):
            file_path = uploaded_file.name
        else:
            # å…¼å®¹å…¶ä»–å¯èƒ½çš„è¿”å›æ ¼å¼
            file_path = str(uploaded_file)
        
        file_name = os.path.basename(file_path)
        file_ext = os.path.splitext(file_name)[1].lower()
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if file_ext not in ['.pdf', '.srt']:
            return f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}\n\næ”¯æŒæ ¼å¼: PDF (.pdf) æˆ– å­—å¹•æ–‡ä»¶ (.srt)"
        
        # å¤„ç†æ–‡ä»¶
        if file_ext == '.pdf':
            chunks, source_name = preprocess_pdf(file_path)
        else:  # .srt
            chunks, source_name = preprocess_srt(file_path)
        
        if not chunks:
            return f"âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼šæœªèƒ½ä» {file_name} ä¸­æå–åˆ°æœ‰æ•ˆå†…å®¹"
        
        # æ·»åŠ åˆ°å‘é‡åº“
        chunk_count = add_documents_to_vectorstore(chunks, source_name)
        
        # è¿”å›æˆåŠŸæ¶ˆæ¯
        result = (
            f"âœ… **æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼**\n\n"
            f"ğŸ“„ **æ–‡ä»¶å**: {file_name}\n"
            f"ğŸ“Š **å¤„ç†ç»“æœ**: ç”Ÿæˆ {chunk_count} ä¸ªæ–‡æœ¬åˆ†å—\n"
            f"ğŸ’¾ **å­˜å‚¨ä½ç½®**: {OUTPUT_DIR}\n"
            f"ğŸ” **å‘é‡åº“**: å·²æ›´æ–°ï¼Œç°åœ¨å¯ä»¥åŸºäºæ­¤æ–‡ä»¶å†…å®¹è¿›è¡Œé—®ç­”å’Œç»ƒä¹ ç”Ÿæˆ\n\n"
            f"ğŸ’¡ **æç¤º**: ä½ ç°åœ¨å¯ä»¥åœ¨ã€Œè¯¾ç¨‹é—®ç­”ã€æˆ–ã€Œä¸ªæ€§åŒ–ç»ƒä¹ ã€æ ‡ç­¾é¡µä¸­ä½¿ç”¨æ–°ä¸Šä¼ çš„å†…å®¹äº†ï¼"
        )
        
        return result
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ **å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯**:\n\n```\n{str(e)}\n```\n\n**è¯¦ç»†é”™è¯¯ä¿¡æ¯**:\n```\n{traceback.format_exc()}\n```"
        print(f"æ–‡ä»¶ä¸Šä¼ é”™è¯¯: {traceback.format_exc()}")
        return error_msg


# ------------------- 6. æµ‹è¯• RAGï¼ˆç£ç›˜ç¼“å­˜ + è·¨è¿›ç¨‹å‘½ä¸­ï¼‰ -------------------
query = "è§£é‡Šé¡¹ç›®é˜¶æ®µï¼ˆProject Phaseï¼‰å’Œé¡¹ç›®ç”Ÿå‘½å‘¨æœŸï¼ˆProject Life Cycleï¼‰çš„æ¦‚å¿µï¼Œå¹¶åŒºåˆ†é¡¹ç›®å¼€å‘ä¸äº§å“å¼€å‘ã€‚"

try:
    # 1ï¼‰æ£€ç´¢è°ƒè¯•ï¼ˆä»…åœ¨ç¼“å­˜æœªå‘½ä¸­æ—¶æ‰§è¡Œï¼‰

    retrieved_docs = retriever.invoke(query)
    print("\n=== æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆMMR, k=15ï¼‰ ===")
    for i, doc in enumerate(retrieved_docs):
        source = doc.metadata.get("source", "")
        if source:
            chunk_id = os.path.splitext(source)[0]  # å¾—åˆ° chunk_3
        else:
            chunk_id = f"unknown_{i}"
        preview = doc.page_content.replace("\n", " ")[:120]
        print(f"Doc {i} ({chunk_id}): {preview}...")


    start_total = time.time()

    # === ç£ç›˜ç¼“å­˜ RAG è°ƒç”¨ ===
    answer = cached_rag(query)

    response_time = time.time() - start_total

    # 2ï¼‰æ‰“å°ç­”æ¡ˆ + å“åº”æ—¶é—´
    print("\n" + "="*70)
    print("DeepSeek RAG å›ç­”:")
    print(answer)
    print(f"æ€»å“åº”æ—¶é—´: {response_time:.3f} ç§’")
    print("="*70)

    # 3ï¼‰æ‰“å°ç¼“å­˜çŠ¶æ€
    normalized = query.strip().lower()
    cache_key = f"rag_v1:{hashlib.md5(normalized.encode()).hexdigest()}"
    print(f"ç¼“å­˜é”®: {cache_key[-12:]}")
    print(f"ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    print(f"å½“å‰ç¼“å­˜å¤§å°: {len(rag_cache)} æ¡")

    # 4ï¼‰å¯é€‰ï¼šæ‰“å°ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
    if cache_key in rag_cache:
        print("ç¼“å­˜çŠ¶æ€: å‘½ä¸­ï¼ˆç¬¬äºŒæ¬¡è¿è¡Œè„šæœ¬å°†ç›´æ¥è¯»å–ï¼‰")
    else:
        print("ç¼“å­˜çŠ¶æ€: æœªå‘½ä¸­ï¼ˆå·²å†™å…¥ç£ç›˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†å‘½ä¸­ï¼‰")

except Exception as e:
    print(f"é—®ç­”æµ‹è¯•å¤±è´¥: {str(e)}")
    raise

# ------------------- 7. Gradio UIï¼ˆå¼‚æ­¥ + è®¡æ—¶ï¼‰ -------------------
async def ask_question(question: str) -> str:
    try:
        loop = asyncio.get_event_loop()
        start = time.time()

        # 1. è·å– LLM çš„åŸå§‹å›ç­”
        full_response = await loop.run_in_executor(None, lambda: qa_chain.run(question))
        elapsed = time.time() - start

        # 2. ä½¿ç”¨ã€æ–°æ­£åˆ™è¡¨è¾¾å¼ã€‘æå–æ¥æº ID
        # è§£é‡Šï¼šåŒ¹é… chunk_ å¼€å¤´ï¼Œåé¢è·Ÿç€ å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€ç‚¹ã€æ¨ªæ  æˆ– ä¸­æ–‡
        pattern = r"(chunk_[\w\.\-\u4e00-\u9fff]+)"
        sources = re.findall(pattern, full_response)

        # å»é‡å¹¶æ ¼å¼åŒ–æ¥æº
        unique_sources = list(set(sources))

        # 3. æ„é€ æœ€ç»ˆæ˜¾ç¤ºçš„æ–‡æœ¬


        display_text = f"ğŸ’¡ **å›ç­”**:\n{full_response}\n\n"
        display_text += f"â±ï¸ **è€—æ—¶**: {elapsed:.2f} ç§’\n"

        if unique_sources:
            display_text += f"ğŸ“š **æ£€æµ‹åˆ°çš„æ¥æºæ–‡ä»¶**: {', '.join(unique_sources)}"
        else:
            display_text += "âš ï¸ æœªæ£€æµ‹åˆ°æ˜ç¡®çš„æ¥æºå¼•ç”¨ (å¯èƒ½æ˜¯é€šç”¨çŸ¥è¯†å›ç­”)"

        return display_text

    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}"

# ------------------- 7. Gradio UI (å‡çº§ç‰ˆï¼šå¤šåŠŸèƒ½é¢æ¿) -------------------

# Tab 1: è¯¾ç¨‹é—®ç­”ç•Œé¢
qa_interface = gradio.Interface(
    fn=ask_question,
    inputs=gradio.Textbox(label="ğŸ’¬ è¯¾ç¨‹æé—®", placeholder="ä¾‹å¦‚ï¼šæ•æ·å¼€å‘çš„æ ¸å¿ƒä»·å€¼è§‚æ˜¯ä»€ä¹ˆï¼Ÿ", lines=2),
    outputs=gradio.Markdown(label="ğŸ¤– AI å›ç­”"), # ä½¿ç”¨ Markdown æ¸²æŸ“å¯Œæ–‡æœ¬
    allow_flagging="never",
    description="**åŸºäº RAG æŠ€æœ¯**ï¼šç²¾å‡†æ£€ç´¢è¯¾ç¨‹è®²ä¹‰ä¸è§†é¢‘å­—å¹•ï¼Œæä¾›å¸¦æº¯æºçš„ä¸“ä¸šè§£ç­”ã€‚"
)

# Tab 2: ç»ƒä¹ ç”Ÿæˆç•Œé¢
quiz_interface = gradio.Interface(
    fn=generate_quiz_func,
    inputs=gradio.Textbox(label="ğŸ¯ è¾“å…¥çŸ¥è¯†ç‚¹", placeholder="ä¾‹å¦‚ï¼šScrum æµç¨‹ / ç€‘å¸ƒæ¨¡å‹ / é£é™©ç®¡ç†", lines=1),
    outputs=gradio.Markdown(label="ğŸ“ ç”Ÿæˆçš„ç»ƒä¹ é¢˜"),
    allow_flagging="never",
    description="**ä¸ªæ€§åŒ–ç»ƒä¹ **ï¼šè¾“å…¥ä½ æƒ³å¤ä¹ çš„çŸ¥è¯†ç‚¹ï¼ŒAI å°†åŸºäºè¯¾ç¨‹èµ„æ–™ä¸ºä½ ç”Ÿæˆä¸€é“å•é€‰é¢˜åŠè§£æã€‚"
)

# Tab 3: æ–‡ä»¶ä¸Šä¼ ç•Œé¢
upload_interface = gradio.Interface(
    fn=handle_file_upload,
    inputs=gradio.File(
        label="ğŸ“¤ ä¸Šä¼ è¯¾ç¨‹æ–‡ä»¶",
        file_types=[".pdf", ".srt"]
    ),
    outputs=gradio.Markdown(label="ğŸ“‹ å¤„ç†ç»“æœ"),
    allow_flagging="never",
    description="**æ–‡ä»¶ä¸Šä¼ **ï¼šä¸Šä¼  PDF è®²ä¹‰æˆ– SRT å­—å¹•æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å¤„ç†å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­ï¼Œæ”¯æŒåç»­é—®ç­”å’Œç»ƒä¹ ç”Ÿæˆã€‚"
)

# ä¸»ç¨‹åºï¼šä½¿ç”¨ TabbedInterface ç»„åˆä¸‰ä¸ªåŠŸèƒ½
demo = gradio.TabbedInterface(
    [qa_interface, quiz_interface, upload_interface],
    ["ğŸ“š è¯¾ç¨‹é—®ç­”", "âœï¸ ä¸ªæ€§åŒ–ç»ƒä¹ ", "ğŸ“¤ æ–‡ä»¶ä¸Šä¼ "],
    title="ğŸ“ LearnMate ä¸ªæ€§åŒ–å­¦ä¹ ä¼™ä¼´ (MVP Alpha)",
    theme="soft"
)

if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ LearnMate Web æœåŠ¡...")
    demo.launch(share=True)